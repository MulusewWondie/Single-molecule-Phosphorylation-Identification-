{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80f8b0ea-3df7-4322-9425-535d43ad0eee",
   "metadata": {},
   "source": [
    "### 1D-CNN model for phosphorylation identification at the single molecule level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820e5f0e-1af3-45b4-9655-2c5945db380f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dropout, Dense, BatchNormalization\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.metrics import Precision, Recall, AUC\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import confusion_matrix, roc_curve\n",
    "import seaborn as sns\n",
    "from itertools import cycle\n",
    "\n",
    "\n",
    "# -------------------- Utility Functions --------------------\n",
    "def ensure_dir(directory):\n",
    "    \"\"\"Ensure the directory exists, create it if it does not.\"\"\"\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "def plot_metrics(history, dataset_name, save_path):\n",
    "    \"\"\"Plot training loss and accuracy and save the figure.\"\"\"\n",
    "    ensure_dir(save_path)\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 6), dpi=800)\n",
    "    \n",
    "    # Increase font sizes\n",
    "    font_size = 18\n",
    "    tick_size = 16\n",
    "    legend_size = 14\n",
    "\n",
    "    # Plot Loss\n",
    "    ax1.set_xlabel('Epoch', fontsize=font_size, fontweight='bold')\n",
    "    ax1.set_ylabel('Loss', color='tab:blue', fontsize=font_size, fontweight='bold')\n",
    "    ax1.plot(history.epoch, history.history['loss'], label='Train Loss', color='tab:blue', linestyle='-')\n",
    "    ax1.tick_params(axis='y', labelcolor='tab:blue', labelsize=tick_size)\n",
    "    ax1.tick_params(axis='x', labelsize=tick_size)\n",
    "\n",
    "    # Plot Accuracy\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.set_ylabel('Accuracy', color='tab:green', fontsize=font_size, fontweight='bold')\n",
    "    ax2.plot(history.epoch, history.history['accuracy'], label='Train Accuracy', color='tab:green', linestyle='--')\n",
    "    ax2.tick_params(axis='y', labelcolor='tab:green', labelsize=tick_size)\n",
    "\n",
    "    # Title\n",
    "    plt.title(f'Training Loss and Accuracy - {dataset_name}', fontsize=font_size + 2, fontweight='bold')\n",
    "\n",
    "    # Grid and Layout\n",
    "    fig.tight_layout()\n",
    "    plt.grid(False)\n",
    "\n",
    "    # Save the Figure\n",
    "    plt.savefig(os.path.join(save_path, f\"{dataset_name}_training_metrics.png\"))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_combined_roc(y_true_list, y_score_list, dataset_names, save_path):\n",
    "    \"\"\"Plot combined ROC curves and save the figure.\"\"\"\n",
    "    ensure_dir(save_path)\n",
    "    plt.figure(figsize=(9, 7))\n",
    "\n",
    "    # Increase font sizes\n",
    "    font_size = 18\n",
    "    tick_size = 16\n",
    "    legend_size = 14\n",
    "\n",
    "    colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'darkgreen', 'red', 'purple'])\n",
    "\n",
    "    for y_true, y_score, dataset_name, color in zip(y_true_list, y_score_list, dataset_names, colors):\n",
    "        fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "        plt.plot(fpr, tpr, color=color, lw=2, label=f'{dataset_name}')\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    \n",
    "    # Axis Labels\n",
    "    plt.xlabel('False Positive Rate', fontsize=font_size, fontweight='bold')\n",
    "    plt.ylabel('True Positive Rate', fontsize=font_size, fontweight='bold')\n",
    "    \n",
    "    # Title\n",
    "    plt.title('Combined ROC Curve for All Datasets', fontsize=font_size + 2, fontweight='bold')\n",
    "    \n",
    "    # Legend\n",
    "    plt.legend(loc=\"lower right\", fontsize=legend_size)\n",
    "\n",
    "    # Grid\n",
    "    plt.grid(False)\n",
    "\n",
    "    # Save the Figure\n",
    "    plt.savefig(os.path.join(save_path, \"combined_roc_curve.png\"))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(true_labels, pred_labels, dataset_name, evaluation_type, class_names, save_path):\n",
    "    \"\"\"Plot and save confusion matrix.\"\"\"\n",
    "    ensure_dir(save_path)\n",
    "    cm = confusion_matrix(true_labels, pred_labels)\n",
    "    cm_percentage = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100  # Convert to percentages\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm_percentage, annot=True, fmt='.2f', cmap=\"Blues\", cbar=False,\n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.ylabel('True label')\n",
    "    plt.title(f'Confusion Matrix - {dataset_name} ({evaluation_type})')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_path, f\"{dataset_name}_{evaluation_type}_confusion_matrix.png\"))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# -------------------- Data Loading --------------------\n",
    "def load_spectrum_file(dir_path):\n",
    "    \"\"\"Load spectrum data from the given directory.\"\"\"\n",
    "    data = []\n",
    "    label = []\n",
    "    file_dir_list = os.listdir(dir_path)\n",
    "    for file_dir in file_dir_list:\n",
    "        file_list = os.listdir(os.path.join(dir_path, file_dir))\n",
    "        for filename in file_list:\n",
    "            file_path = os.path.join(dir_path, file_dir, filename)\n",
    "            x, y = np.loadtxt(file_path, dtype=float, comments='#', delimiter='\\t', unpack=True)\n",
    "            data.append(y)\n",
    "            label.append(file_dir)\n",
    "    return np.array(data), label\n",
    "\n",
    "\n",
    "# -------------------- Model Training and Evaluation --------------------\n",
    "def run_analysis(train_path, post_eval_path, dataset_name, save_path):\n",
    "    \"\"\"Train model, evaluate on validation and post-evaluation sets, and save results.\"\"\"\n",
    "    print(f\"\\nLoading dataset from: {train_path}\")\n",
    "    data, label = load_spectrum_file(train_path)\n",
    "    data = data.reshape((data.shape[0], data.shape[1], 1))\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    int_labels = le.fit_transform(label)\n",
    "    labels = int_labels\n",
    "    class_names = le.classes_\n",
    "\n",
    "    train_data, val_data, train_label, val_label = train_test_split(\n",
    "        data, labels, train_size=0.7, test_size=0.3, stratify=labels, shuffle=True\n",
    "    )\n",
    "\n",
    "    # Define Model\n",
    "    model = Sequential()\n",
    "    model.add(tf.keras.layers.InputLayer(input_shape=(data.shape[1], 1)))\n",
    "    conv_layers = [16,32,64]\n",
    "    reg_strength = 1e-4\n",
    "\n",
    "    for cl in conv_layers:\n",
    "        model.add(Conv1D(cl, 3, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(reg_strength)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Conv1D(cl, 3, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(reg_strength)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling1D(2))\n",
    "        model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(reg_strength)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(reg_strength)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=1e-3),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', Precision(), Recall(), AUC(name='auc')]\n",
    "    )\n",
    "\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_auc', patience=10, mode='max', restore_best_weights=True, verbose=2\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        train_data, train_label,\n",
    "        validation_data=(val_data, val_label),\n",
    "        batch_size=16,\n",
    "        epochs=50,\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    plot_metrics(history, dataset_name, save_path)\n",
    "\n",
    "    print(\"\\nEvaluating on validation set...\")\n",
    "    val_pred_probs = model.predict(val_data, batch_size=32).ravel()\n",
    "    val_pred_classes = (val_pred_probs > 0.5).astype(int)\n",
    "    val_true_classes = val_label\n",
    "\n",
    "    plot_confusion_matrix(val_true_classes, val_pred_classes, dataset_name, \"Validation\", class_names, save_path)\n",
    "\n",
    "    print(f\"\\nLoading post-evaluation dataset from: {post_eval_path}\")\n",
    "    post_data, post_label = load_spectrum_file(post_eval_path)\n",
    "    post_data = post_data.reshape((post_data.shape[0], post_data.shape[1], 1))\n",
    "    post_label_encoded = le.transform(post_label)\n",
    "\n",
    "    print(\"\\nEvaluating on post-evaluation set...\")\n",
    "    post_pred_probs = model.predict(post_data, batch_size=32).ravel()\n",
    "    post_pred_classes = (post_pred_probs > 0.5).astype(int)\n",
    "\n",
    "    plot_confusion_matrix(post_label_encoded, post_pred_classes, dataset_name, \"Post-Evaluation\", class_names, save_path)\n",
    "\n",
    "    return val_true_classes, val_pred_probs, post_label_encoded, post_pred_probs\n",
    "\n",
    "\n",
    "# -------------------- Run and Save Results --------------------\n",
    "save_path = \"\"  # Directory to save all generated figures\n",
    "\n",
    "datasets = [\n",
    "    {\"train_path\": \"\",\n",
    "     \"post_eval_path\": \"\",\n",
    "     \"name\": \"Cit-free\"},\n",
    "    {\"train_path\": \"\",\n",
    "     \"post_eval_path\": \"\",\n",
    "     \"name\": \"Cit-affected\"}\n",
    "]\n",
    "\n",
    "val_labels_list = []\n",
    "val_probs_list = []\n",
    "post_labels_list = []\n",
    "post_probs_list = []\n",
    "dataset_names = []\n",
    "\n",
    "for dataset in datasets:\n",
    "    val_label, val_probs, post_label, post_probs = run_analysis(\n",
    "        dataset[\"train_path\"], dataset[\"post_eval_path\"], dataset[\"name\"], save_path\n",
    "    )\n",
    "    val_labels_list.append(val_label)\n",
    "    val_probs_list.append(val_probs)\n",
    "    post_labels_list.append(post_label)\n",
    "    post_probs_list.append(post_probs)\n",
    "    dataset_names.append(dataset[\"name\"] + \" (Validation)\")\n",
    "    dataset_names.append(dataset[\"name\"] + \" (Post-Evaluation)\")\n",
    "\n",
    "# Save combined ROC curve\n",
    "plot_combined_roc(val_labels_list + post_labels_list, val_probs_list + post_probs_list, dataset_names, save_path)\n",
    "model_save_path =\"\"\n",
    "\n",
    "# Save the model\n",
    "model.save(model_save_path)\n",
    "print(f\"Model saved successfully at {model_save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e4d707-499d-43b7-9245-d0c79172740e",
   "metadata": {},
   "source": [
    "### 1D-Gradient Feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce021da1-9515-4616-b3f1-115a52b3525c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, numpy as np, tensorflow as tf, matplotlib.pyplot as plt\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "# ────────── paths ──────────\n",
    "DATA_DIR   = r\"\" #enter the directory of the dataset\n",
    "MODEL_PATH = r\"\" # directory of model\n",
    "\n",
    "# ────────── helper functions ───────────────────────────────────────────\n",
    "def load_spectra_and_labels(root):\n",
    "    data, labels, raman = [], [], None\n",
    "    for cls in sorted(os.listdir(root)):\n",
    "        sub = os.path.join(root, cls)\n",
    "        if not os.path.isdir(sub): continue\n",
    "        for fn in os.listdir(sub):\n",
    "            if fn.lower().endswith('.txt'):\n",
    "                x, y = np.loadtxt(os.path.join(sub, fn), delimiter='\\t', unpack=True)\n",
    "                if raman is None: raman = x\n",
    "                data.append(y)\n",
    "                labels.append(cls)\n",
    "    return np.vstack(data), np.array(labels), raman\n",
    "\n",
    "def gradient_saliency_x_input(model, X, positive=True):\n",
    "    \"\"\"\n",
    "    positive=True  ⇒ returns | ∂p/∂X  * X |     (p = P(pSer))\n",
    "    positive=False ⇒ returns | ∂(1−p)/∂X * X |\n",
    "    \"\"\"\n",
    "    Xtf = tf.convert_to_tensor(X, tf.float32)\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(Xtf)\n",
    "        p = model(Xtf)[:,0]                     \n",
    "        score =      p if positive else (1.0 - p)\n",
    "    grads = tape.gradient(score, Xtf).numpy().squeeze()  \n",
    "    X_np  = X.squeeze()                                   \n",
    "    sal   = np.abs(grads * X_np)                          # elementwise\n",
    "    return sal\n",
    "\n",
    "\n",
    "def peak_frequency(X, prom=0.4, h=0.5):\n",
    "    Xf = X.reshape(X.shape[0], -1)\n",
    "    n, m = Xf.shape\n",
    "    present = np.zeros((n, m), bool)\n",
    "    for i in range(n):\n",
    "        y = (Xf[i] - Xf[i].min()) / Xf[i].ptp()\n",
    "        pk, _ = find_peaks(y, prominence=prom, height=h)\n",
    "        present[i, pk] = True\n",
    "    return present.sum(0) / n\n",
    "\n",
    "# ────────── main ───────────────────────────────────────────────────────\n",
    "model       = tf.keras.models.load_model(MODEL_PATH)\n",
    "data, labels, raman = load_spectra_and_labels(DATA_DIR)\n",
    "X           = data[..., None]               \n",
    "y           = (labels == \"pSer Cit-affected\").astype(int)\n",
    "\n",
    "G_ser  = gradient_saliency_x_input(model, X[y==0], positive=False)\n",
    "G_pser = gradient_saliency_x_input(model, X[y==1], positive=True)\n",
    "\n",
    "ser_w  = G_ser.mean(0)\n",
    "pser_w = G_pser.mean(0)\n",
    "ser_w /= ser_w.max()\n",
    "pser_w /= pser_w.max()\n",
    "\n",
    "\n",
    "# per-class peak frequencies\n",
    "freq_ser  = peak_frequency(X[y==0])\n",
    "freq_pser = peak_frequency(X[y==1])\n",
    "\n",
    "# ────────── plot ───────────────────────────────────────────────────────\n",
    "fig, (ax1, ax2) = plt.subplots(2,1, sharex=True, figsize=(12,6), dpi=200,\n",
    "                               gridspec_kw={'hspace':0.05})\n",
    "w = raman[1] - raman[0]\n",
    "\n",
    "# Ser\n",
    "ax1.plot(raman, ser_w,  color='C0', lw=1.6)\n",
    "ax1.set_ylabel(\"Normalized\\nFeature Weight\", color='C0', fontsize=12, fontweight='bold')  # Add newline for wrapping\n",
    "ax1.tick_params(axis='y', labelcolor='C0')\n",
    "\n",
    "# Adjust y-axis limits for ax1 to add space\n",
    "y_min_ax1, y_max_ax1 = ax1.get_ylim() \n",
    "ax1.set_ylim(y_min_ax1, y_max_ax1 * 1.1)  \n",
    "\n",
    "ax1b = ax1.twinx()\n",
    "ax1b.bar(raman, freq_ser, width=w, color='0.25', alpha=0.5)\n",
    "ax1b.set_ylabel(\"Peak Occurrence\\nFrequency\", color='0.25', fontsize=12, fontweight='bold')  # Add newline for wrapping\n",
    "ax1b.tick_params(axis='y', labelcolor='0.25')\n",
    "\n",
    "# Adjust y-axis limits for ax1b to add space if necessary\n",
    "y_min_ax1b, y_max_ax1b = ax1b.get_ylim()  # Get current y-axis limits for twin axis\n",
    "ax1b.set_ylim(y_min_ax1b, y_max_ax1b * 1.1)  # Increase the upper limit by 10%\n",
    "\n",
    "# pSer\n",
    "ax2.plot(raman, pser_w, color='C3', lw=1.6)\n",
    "ax2.set_ylabel(\"Normalized\\nFeature Weight\", color='C3', fontsize=12, fontweight='bold')  # Add newline for wrapping\n",
    "ax2.tick_params(axis='y', labelcolor='C3')\n",
    "\n",
    "# Adjust y-axis limits for ax2 to add space\n",
    "y_min_ax2, y_max_ax2 = ax2.get_ylim()  # Get current y-axis limits\n",
    "ax2.set_ylim(y_min_ax2, y_max_ax2 * 1.1)  # Increase the upper limit by 10%\n",
    "\n",
    "ax2b = ax2.twinx()\n",
    "ax2b.bar(raman, freq_pser, width=w, color='0.25', alpha=0.5)\n",
    "ax2b.set_ylabel(\"Peak Occurrence\\nFrequency\", color='0.25', fontsize=12, fontweight='bold')  # Add newline for wrapping\n",
    "ax2b.tick_params(axis='y', labelcolor='0.25')\n",
    "\n",
    "# Adjust y-axis limits for ax2b to add space if necessary\n",
    "y_min_ax2b, y_max_ax2b = ax2b.get_ylim()  # Get current y-axis limits for twin axis\n",
    "ax2b.set_ylim(y_min_ax2b, y_max_ax2b * 1.1)  # Increase the upper limit by 10%\n",
    "\n",
    "ax2.set_xlabel(\"Raman Shift (cm$^{-1}$)\", fontsize=12, fontweight='bold')\n",
    "ax2.set_xlim(raman[0], raman[-1])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "out_dir = r\"C\" # out directory\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "# save high-res figure\n",
    "fname = os.path.join(out_dir, '')\n",
    "fig.savefig(fname, dpi=300, format='png', bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
